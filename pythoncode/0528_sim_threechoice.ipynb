{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#lqr controller plotting\n",
    "#potentially fix with upgrade of tensorflow?? check current version first in case want to revert.\n",
    "\n",
    "\n",
    "#notes\n",
    "#already integrated lqr control and plotting is working. cleaned up to make code more concise. interpretations pasted at end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/nana/opt/anaconda3/envs/latent_circuit/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# the tensorflow version of the wong/wang dynamics that only uses a TF version of the controller\n",
    "#the rest is in numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tf_funs import * #EKF, and helper in tensorflow\n",
    "from threechoice_dynamics import * #the gradients for wong/wang dynamics\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from scipy.linalg import solve_discrete_are\n",
    "\n",
    "\n",
    "true_nontf = lambda x,c: grad_threechoice(x[0],x[1],dt,c)\n",
    "target_nontf = lambda x: grad_threechoice_healthy(x[0],x[1],dt)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__) #1.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mean-only control\n",
      "Responsiveness factor:  1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bee90314938c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;31m# Update actual dynamics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mgrad_cont\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_threechoice_tf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxvec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxvec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontall\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m             \u001b[0mgrad_uc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrue_nontf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_ucvec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mgrad_targ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_nontf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_targvec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/latent_circuit/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/latent_circuit/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/latent_circuit/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/latent_circuit/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/latent_circuit/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1346\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m   1350\u001b[0m                                       target_list, run_metadata)\n",
      "\u001b[0;32m~/opt/anaconda3/envs/latent_circuit/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pi = math.pi\n",
    "\n",
    "session_conf = tf.ConfigProto(\n",
    "      intra_op_parallelism_threads=4,\n",
    "      inter_op_parallelism_threads=4)\n",
    "sess = tf.Session(config=session_conf)\n",
    "\n",
    "# Define the LQR Controller Class\n",
    "class LQRController:\n",
    "    def __init__(self, A, B, Q, R, responsiveness_factor):\n",
    "        self.A = A\n",
    "        self.B = B\n",
    "        self.Q = Q\n",
    "        self.R = R\n",
    "        self.responsiveness_factor = responsiveness_factor\n",
    "        self.K = self.calculate_lqr_gain()\n",
    "\n",
    "    def calculate_lqr_gain(self):\n",
    "        # Solve the Riccati equation\n",
    "        P = solve_discrete_are(self.A, self.B, self.Q, self.R)\n",
    "        # Calculate the LQR gain\n",
    "        K = np.linalg.inv(self.R + self.B.T @ P @ self.B) @ (self.B.T @ P @ self.A)\n",
    "        return K\n",
    "\n",
    "    def calculate_control(self, x):\n",
    "        # Calculate the control signal\n",
    "        u = -self.K @ x\n",
    "        return u * self.responsiveness_factor\n",
    "\n",
    "    def apply_control(self, x, x_target):\n",
    "        # Calculate the state error\n",
    "        error = x - x_target\n",
    "        # Get the control signal\n",
    "        control_signal = self.calculate_control(error)\n",
    "        return control_signal\n",
    "\n",
    "# Define system dynamics matrices for LQR\n",
    "A = np.array([[1.0, 0.1], [0, 1.0]])  # Example system matrix\n",
    "B = np.array([[0.0], [0.1]])          # Example input matrix\n",
    "Q = np.eye(2)                         # State cost matrix\n",
    "R = np.eye(1)                         # Control cost matrix\n",
    "\n",
    "# Build graphs\n",
    "xdim = 2  # State dimension\n",
    "udim = 2  # Control dimension\n",
    "X_est = tf.placeholder(shape=(xdim), dtype=tf.float32, name='X_est')  # State estimate\n",
    "PI_est = tf.placeholder(shape=(xdim, xdim), dtype=tf.float32, name='PI_est')  # Estimated covariance\n",
    "Y_tp1 = tf.placeholder(shape=(xdim), dtype=tf.float32, name='Y_tp1')  # Most recent observation\n",
    "Control = tf.placeholder(shape=(udim), dtype=tf.float32, name='Control')\n",
    "\n",
    "# Define the noise for the system\n",
    "dt = 1.0e-4\n",
    "gamma = 1.0e-4\n",
    "sigsstate = (1./dt)*(1e-9)\n",
    "sigsobs = 1e-6\n",
    "\n",
    "Q_tf = tf.constant(sigsstate * np.eye(xdim), dtype=tf.float32, name='Q')  # State noise covariance\n",
    "R_tf = tf.constant(sigsobs * np.eye(xdim), dtype=tf.float32, name='R')  # Observation noise covariance\n",
    "\n",
    "# Define your dynamics and other graphs...\n",
    "true_model_est = grad_threechoice_tf(X_est[0], X_est[1], dt, Control, 1.0)\n",
    "true_model_est_null = grad_threechoice_tf(X_est[0], X_est[1], dt, [0., 0.], 1.0)  # State est. gradient null control\n",
    "target_model_est = grad_threechoice_healthy_tf(X_est[0], X_est[1], dt, 1.0)  # State est. target dynamics\n",
    "\n",
    "X_plus, PI_plus = EKF(X_est, Y_tp1, PI_est, true_model_est, true_model_est_null, Q_tf, R_tf, xdim, dt)\n",
    "\n",
    "# Myopic controller\n",
    "useMO = 1\n",
    "if useMO == 1:\n",
    "    print('Using mean-only control')\n",
    "    Cnew = myopicController_meanonly(X_est, PI_est, Control, gamma, true_model_est, true_model_est_null, target_model_est, xdim, udim)\n",
    "else:\n",
    "    print('Using full myopic control, but non-Jacobian-differential form')\n",
    "    Cnew = myopicController_noBdiff(X_est, PI_est, Control, gamma, true_model_est, true_model_est_null, target_model_est, xdim, udim)\n",
    "\n",
    "# Covariance prediction update graph\n",
    "Ak = dynamics_linearized(X_est, true_model_est_null, xdim)\n",
    "\n",
    "# The full loss function, not just loss of mean values\n",
    "loss_tf = loss_full(X_est, PI_est, true_model_est, target_model_est)\n",
    "\n",
    "# Begin specifics of the simulation\n",
    "T = 1000  # Number of steps\n",
    "ns = 1  # Number of samples\n",
    "\n",
    "# Make the numpy version of noise\n",
    "statenoise = np.random.normal(0, sigsstate ** 0.5, [xdim, T, ns])\n",
    "obsnoise = np.random.normal(0, sigsobs ** 0.5, [xdim, T, ns])\n",
    "G = dt ** 0.5 * np.eye(xdim)  # System noise matrix, for covariance prediction\n",
    "\n",
    "# Vectors of interest\n",
    "x_estvec = np.zeros((xdim, T, ns))  # State estimate\n",
    "x_ucvec = np.zeros((xdim, T, ns))  # Uncontrolled state\n",
    "xvec = np.zeros((xdim, T, ns))  # Actual controlled state\n",
    "yvec = np.zeros((xdim, T, ns))  # Observations related to state\n",
    "x_targvec = np.zeros((xdim, T, ns))  # Target state\n",
    "PI_estvec = np.zeros((xdim, xdim, T, ns))  # Estimated covariance\n",
    "contall = np.zeros((udim, T, ns))  # Control\n",
    "\n",
    "# Initialize arrays to store LQR controller data\n",
    "contall_lqr = np.zeros((udim, T, ns))\n",
    "x_estvec_lqr = np.zeros((xdim, T, ns))  # LQR state estimate\n",
    "loss_lqr = np.zeros((4, T, ns))  # LQR loss\n",
    "\n",
    "# Loss function values in vector form as (full loss, mean-only loss, jacobian loss term, Hessian term)\n",
    "loss = np.zeros((4, T, ns))\n",
    "loss_nocont = np.zeros((4, T, ns))\n",
    "loss_true = np.zeros((T, ns))\n",
    "initvals = np.zeros((xdim, ns))\n",
    "\n",
    "lag = 1  # 1+number of steps difference between observations and state estimate\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "responsiveness_factors = [1.0, 0.75, 0.5, 0.25]\n",
    "\n",
    "for resp_factor in responsiveness_factors:\n",
    "    print('Responsiveness factor: ', resp_factor)\n",
    "    # Initialize the LQR controller with the current responsiveness factor\n",
    "    lqr_controller = LQRController(A, B, Q, R, resp_factor)\n",
    "\n",
    "    for m in range(ns):\n",
    "        sess.run(init)\n",
    "        x_init = np.random.uniform(0.1, 0.2, (2,))\n",
    "        initvals[:, m] = x_init\n",
    "        PI_init = [[1.0e-6, 0.], [0., 1.0e-6]]  # Initial covariance\n",
    "        c_init = [0., 0.]\n",
    "\n",
    "        xest_k = x_init\n",
    "        pi_k = PI_init\n",
    "        c_k = c_init\n",
    "        x_k = x_init\n",
    "        x_targ_k = x_init\n",
    "        ykp1 = np.array(x_init)\n",
    "\n",
    "        x_estvec[:, 0, m] = x_init\n",
    "        xvec[:, 0, m] = x_init\n",
    "        x_ucvec[:, 0, m] = x_init\n",
    "        x_targvec[:, 0, m] = x_init\n",
    "        PI_estvec[:, :, 0, m] = PI_init\n",
    "        x_estvec_lqr[:, 0, m] = x_init\n",
    "\n",
    "        # Propagate lag-steps ahead before starting state estimation\n",
    "        for k in range(1, lag):\n",
    "            # Update actual dynamics\n",
    "            grad_cont = true_nontf(xvec[:, k - 1, m], c_init)\n",
    "            grad_targ = target_nontf(x_targvec[:, k - 1, m])\n",
    "            grad_uc = true_nontf(x_ucvec[:, k - 1, m], c_init)\n",
    "\n",
    "            xvec[:, k, m] = xvec[:, k - 1, m] + grad_cont + statenoise[:, k, m]\n",
    "            x_ucvec[:, k, m] = x_ucvec[:, k - 1, m] + grad_uc + statenoise[:, k, m]\n",
    "            x_targvec[:, k, m] = x_targvec[:, k - 1, m] + grad_targ + statenoise[:, k, m]\n",
    "            yvec[:, k, m] = xvec[:, k, m] + obsnoise[:, k, m]\n",
    "\n",
    "            # Set estimates in beginning lags to initial state\n",
    "            x_estvec[:, k, m] = x_init\n",
    "            PI_estvec[:, :, k, m] = PI_init\n",
    "            x_estvec_lqr[:, k, m] = x_init\n",
    "\n",
    "        for k in range(max(1, lag), T): \n",
    "            # Update actual dynamics\n",
    "            grad_cont = sess.run(grad_threechoice_tf(xvec[:, k - 1, m][0], xvec[:, k - 1, m][1], dt, contall[:, k - 1, m], resp_factor))\n",
    "            grad_uc = true_nontf(x_ucvec[:, k - 1, m], c_init)\n",
    "            grad_targ = target_nontf(x_targvec[:, k - 1, m])\n",
    "            xvec[:, k, m] = xvec[:, k - 1, m] + grad_cont + statenoise[:, k, m]\n",
    "            x_ucvec[:, k, m] = x_ucvec[:, k - 1, m] + grad_uc + statenoise[:, k, m]\n",
    "            x_targvec[:, k, m] = x_targvec[:, k - 1, m] + grad_targ + statenoise[:, k, m]\n",
    "            yvec[:, k, m] = xvec[:, k, m] + obsnoise[:, k, m]\n",
    "\n",
    "            # Run state estimator to update estimate of state k-lag\n",
    "            test = sess.run([X_plus, PI_plus],\n",
    "                            {X_est: x_estvec[:, k - lag, m], \n",
    "                             PI_est: PI_estvec[:, :, k - lag, m], \n",
    "                             Control: contall[:, k - lag, m], Y_tp1: yvec[:, k - lag + 1, m]})\n",
    "            x_estvec[:, k - lag + 1, m] = test[0]\n",
    "            PI_estvec[:, :, k - lag + 1, m] = test[1]\n",
    "            \n",
    "            # Predict lag states in the future to calculate control\n",
    "            x_est_n = x_estvec[:, k - lag + 1, m]\n",
    "            PI_est_n = PI_estvec[:, :, k - lag + 1, m]\n",
    "            \n",
    "            for n in range(1, lag):\n",
    "                # State prediction step\n",
    "                grad_cont = true_nontf(x_est_n, contall[:, k - lag - n, m])\n",
    "\n",
    "                # Covariance prediction step. Calculate jacobian\n",
    "                Ak_n = sess.run(Ak,\n",
    "                                {X_est: x_est_n, PI_est: PI_est_n,\n",
    "                                 Control: contall[:, 0, m], Y_tp1: yvec[:, 0, m]})\n",
    "\n",
    "                x_est_n = x_est_n + grad_cont\n",
    "                PI_est_n = np.matmul(Ak_n, PI_est_n) + np.matmul(PI_est_n, np.transpose(Ak_n)) + np.matmul(\n",
    "                    np.matmul(G, Q), np.transpose(G))\n",
    "\n",
    "            # Run myopic controller using predicted state estimate\n",
    "            c_k_myopic = sess.run(Cnew, {X_est: x_est_n, PI_est: PI_est_n,\n",
    "                                         Control: contall[:, k - 1, m], Y_tp1: yvec[:, k, m]})\n",
    "            \n",
    "            # Run LQR controller\n",
    "            c_k_lqr = lqr_controller.apply_control(x_est_n, x_targ_k)\n",
    "\n",
    "            # Store controls and state estimates\n",
    "            if abs(np.linalg.norm(c_k_myopic)) > 100.:\n",
    "                contall[:, k, m] = contall[:, k - 1, m]\n",
    "                print('Dynamics likely got singular. Hold tight')\n",
    "                print(k)\n",
    "            else:\n",
    "                contall[:, k, m] = c_k_myopic\n",
    "            contall_lqr[:, k, m] = c_k_lqr\n",
    "            x_estvec_lqr[:, k, m] = x_est_n\n",
    "\n",
    "            # Calculate LQR loss\n",
    "            loss_lqr[0, k, m] = np.linalg.norm(true_nontf(x_estvec_lqr[:, k, m], contall_lqr[:, k, m]) -\n",
    "                                               target_nontf(x_estvec_lqr[:, k, m])) ** 2\n",
    "\n",
    "            # Calculate true loss from actual state\n",
    "            loss_true[k - lag + 1, m] = np.linalg.norm(true_nontf(xvec[:, k - lag + 1, m], contall[:, k - lag + 1, m]) -\n",
    "                                                       target_nontf(xvec[:, k - lag + 1, m])) ** 2\n",
    "\n",
    "            # Approximated loss from estimated state\n",
    "            ltest = sess.run(loss_tf, {X_est: x_estvec[:, k - lag + 1, m],\n",
    "                                       PI_est: PI_estvec[:, :, k - lag + 1, m],\n",
    "                                       Control: contall[:, k - lag + 1, m]})\n",
    "            loss[:, k - lag + 1, m] = ltest\n",
    "\n",
    "            # Approximated loss from estimated state, no control\n",
    "            ltest = sess.run(loss_tf, {X_est: x_estvec[:, k - lag + 1, m],\n",
    "                                       PI_est: PI_estvec[:, :, k - lag + 1, m],\n",
    "                                       Control: np.array([0., 0.])})\n",
    "            loss_nocont[:, k - lag + 1, m] = ltest\n",
    "            \n",
    "        # Set final lag estimate values to estimate\n",
    "        for k in range(lag - 1):\n",
    "            x_targvec[:, T - lag + 1 + k, m] = x_targvec[:, T - lag, m]\n",
    "            x_estvec[:, T - lag + 1 + k, m] = x_estvec[:, T - lag, m]\n",
    "            x_ucvec[:, T - lag + 1 + k, m] = x_ucvec[:, T - lag, m]\n",
    "            PI_estvec[:, :, T - lag + 1 + k, m] = PI_estvec[:, :, T - lag, m]\n",
    "            x_estvec_lqr[:, T - lag + 1 + k, m] = x_estvec_lqr[:, T - lag, m]\n",
    "\n",
    "# Plotting\n",
    "# Plot the state trajectories\n",
    "tind = 0\n",
    "plt.plot(x_estvec[0, :, tind], x_estvec[1, :, tind], label='Controlled dynamics (Myopic)')\n",
    "plt.plot(x_targvec[0, :, tind], x_targvec[1, :, tind], label='Target dynamics')\n",
    "plt.plot(x_ucvec[0, :, tind], x_ucvec[1, :, tind], label='Uncontrolled dynamics')\n",
    "plt.plot(x_estvec_lqr[0, :, tind], x_estvec_lqr[1, :, tind], label='Controlled dynamics (LQR)', linestyle='--')\n",
    "plt.legend()\n",
    "plt.xlabel('X_1')\n",
    "plt.ylabel('X_2')\n",
    "plt.title('Controlled Trajectory')\n",
    "plt.show()\n",
    "\n",
    "# Plot the control signals\n",
    "plt.plot(contall[0, :, tind], label='Control on S_1 (Myopic)')\n",
    "plt.plot(contall[1, :, tind], label='Control on S_2 (Myopic)')\n",
    "plt.plot(contall_lqr[0, :, tind], label='Control on S_1 (LQR)', linestyle='--')\n",
    "plt.plot(contall_lqr[1, :, tind], label='Control on S_2 (LQR)', linestyle='--')\n",
    "plt.ylabel('C(t)')\n",
    "plt.title('Control Signals')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the loss function\n",
    "plt.plot(loss[0, :, tind], label='Myopic Control')\n",
    "plt.plot(loss_nocont[0, :, tind], label='No Control')\n",
    "plt.plot(loss_lqr[0, :, tind], label='LQR Control', linestyle='--')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Time-Dependent Loss Function')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's interpret the graphs one by one based on the plots you've provided:\n",
    "\n",
    "1. Loss Function Over Time\n",
    "Y-axis: Loss (could be a measure of deviation from the target state).\n",
    "X-axis: Time steps.\n",
    "Lines:\n",
    "Blue (Myopic Control): The myopic controller's loss over time.\n",
    "Orange (No Control): The loss without any control applied.\n",
    "Green Dashed (LQR Control): The LQR controller's loss over time.\n",
    "Interpretation:\n",
    "The LQR controller (green dashed line) shows an increase in loss initially, indicating an adjustment phase, but it stabilizes and maintains a higher level of loss compared to the myopic control.\n",
    "The myopic control (blue line) starts with low loss and maintains it throughout the time steps.\n",
    "The no control (orange line) maintains a constant loss, indicating no improvement over time.\n",
    "2. Control Signals Over Time\n",
    "Y-axis: Control signal values (C(t)).\n",
    "X-axis: Time steps.\n",
    "Lines:\n",
    "Blue (Control on S_1, Myopic): Control signal for the first state dimension using the myopic controller.\n",
    "Orange (Control on S_2, Myopic): Control signal for the second state dimension using the myopic controller.\n",
    "Green Dashed (Control on S_1, LQR): Control signal for the first state dimension using the LQR controller.\n",
    "Red Dashed (Control on S_2, LQR): Control signal for the second state dimension using the LQR controller.\n",
    "Interpretation:\n",
    "The LQR control signals (green dashed and red dashed lines) show a decreasing trend over time, indicating that the LQR controller is applying less control effort as time progresses.\n",
    "The myopic control signals (blue and orange lines) are more stable, with slight variations indicating consistent control application throughout the time steps.\n",
    "There is a significant difference in the control signals applied by the LQR and myopic controllers, which might explain the differences in their performance.\n",
    "3. State Trajectories\n",
    "X-axis: First state dimension (X_1).\n",
    "Y-axis: Second state dimension (X_2).\n",
    "Lines:\n",
    "Blue (Controlled dynamics, Myopic): State trajectory using the myopic controller.\n",
    "Orange (Target dynamics): Target state trajectory.\n",
    "Green (Uncontrolled dynamics): State trajectory without any control applied.\n",
    "Red Dashed (Controlled dynamics, LQR): State trajectory using the LQR controller.\n",
    "Interpretation:\n",
    "The target dynamics (orange line) represent the desired state trajectory.\n",
    "The uncontrolled dynamics (green line) deviate significantly from the target, showing the natural system behavior without control.\n",
    "The myopic controlled dynamics (blue line) closely follow the target dynamics, indicating effective control.\n",
    "The LQR controlled dynamics (red dashed line) initially follow a different path and eventually stabilize, but they do not align as closely with the target dynamics as the myopic controller.\n",
    "Overall Interpretation:\n",
    "Myopic Control: Shows consistent and effective control by maintaining low loss, stable control signals, and state trajectories closely following the target dynamics.\n",
    "LQR Control: Demonstrates higher initial control effort that decreases over time, leading to higher loss and less alignment with the target dynamics compared to the myopic controller.\n",
    "No Control: Predictably ineffective, with constant loss and uncontrolled state trajectories diverging significantly from the target.\n",
    "These graphs collectively suggest that the myopic controller is more effective in this scenario, maintaining lower loss and better adherence to the target dynamics with stable control efforts. The LQR controller, while initially aggressive, stabilizes but does not perform as well as the myopic controller in maintaining the desired state trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gain Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedback Loop Enhancement: Introduce more sophisticated feedback mechanisms, such as non-linear control laws or gain scheduling, where control parameters change based on the system's state.\n",
    "\n",
    "State Estimation Accuracy: Improve the accuracy of the state estimation by refining the model used in the EKF. Better state estimation can result in more accurate control actions.\n",
    "\n",
    "Control Law Modifications: Alter the structure of the control law itself. For example, introduce derivative control to provide damping or use integral control to eliminate steady-state errors.\n",
    "\n",
    "Adding Heuristics: Include heuristic rules based on domain knowledge that can improve control actions. For instance, if you know certain states are particularly problematic, you could add rules to handle these explicitly.\n",
    "\n",
    "Learning from Past Performance: Implement a learning mechanism that adjusts control parameters based on past performance. This is not predictive control, but rather a retrospective adjustment to improve future performance.\n",
    "\n",
    "Noise Filtering: Improve the noise filtering in the observations fed into the control algorithm. If the control input is noisy, it could lead to erratic behavior.\n",
    "\n",
    "Robust Control Techniques: Employ robust control techniques that are designed to perform well in the face of uncertainty and system variability.\n",
    "\n",
    "Action Smoothing: If the control action is jumping erratically, introduce a smoothing factor to make changes more gradual and prevent sudden shifts.\n",
    "\n",
    "Simulation-Based Optimization: Use simulations to optimize the control parameters for the most common scenarios that the system faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latent_circuit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
